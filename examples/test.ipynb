{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "duplicate parameter name: 'sample_a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     20\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n\u001b[1;32m     22\u001b[0m \u001b[39m@dyw\u001b[39;49m\u001b[39m.\u001b[39;49mdynamize\n\u001b[0;32m---> 23\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mC\u001b[39;49;00m:\n\u001b[1;32m     24\u001b[0m     sample_a \u001b[39m=\u001b[39;49m dyw\u001b[39m.\u001b[39;49mfield(\u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m, is_class\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, constructor_arguments\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m2\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m5\u001b[39;49m})\n\u001b[1;32m     25\u001b[0m     \u001b[39mdef\u001b[39;49;00m \u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, a, sample_a):\n",
      "File \u001b[0;32m~/Work/myprojects/dycode/tests/../dycode/wrappers/full_wrapper.py:38\u001b[0m, in \u001b[0;36mdynamize\u001b[0;34m(cls, inheritence_strict, blend, indicator_prefix)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(\u001b[39mcls\u001b[39;49m)\n",
      "File \u001b[0;32m~/Work/myprojects/dycode/tests/../dycode/wrappers/full_wrapper.py:33\u001b[0m, in \u001b[0;36mdynamize.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mreturn\u001b[39;00m _dynamize(\u001b[39mcls\u001b[39;49m, inheritence_strict, blend, indicator_prefix)\n",
      "File \u001b[0;32m~/Work/myprojects/dycode/tests/../dycode/wrappers/full_wrapper.py:13\u001b[0m, in \u001b[0;36m_dynamize\u001b[0;34m(cls, inheritence_strict, blend, indicator_prefix)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_dynamize\u001b[39m(\n\u001b[1;32m      7\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m      8\u001b[0m     inheritence_strict: \u001b[39mbool\u001b[39m,\n\u001b[1;32m      9\u001b[0m     blend: \u001b[39mbool\u001b[39m,\n\u001b[1;32m     10\u001b[0m     indicator_prefix: th\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     11\u001b[0m ):\n\u001b[1;32m     12\u001b[0m     ret_cls \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     ret_cls \u001b[39m=\u001b[39m _dynamize_fields(\n\u001b[1;32m     14\u001b[0m         ret_cls,\n\u001b[1;32m     15\u001b[0m         inheritence_strict\u001b[39m=\u001b[39;49minheritence_strict,\n\u001b[1;32m     16\u001b[0m         indicator_prefix\u001b[39m=\u001b[39;49mindicator_prefix,\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     ret_cls \u001b[39m=\u001b[39m _dynamize_methods(\n\u001b[1;32m     19\u001b[0m         ret_cls, inheritence_strict\u001b[39m=\u001b[39minheritence_strict, blend\u001b[39m=\u001b[39mblend\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m ret_cls\n",
      "File \u001b[0;32m~/Work/myprojects/dycode/tests/../dycode/wrappers/field_wrapper.py:319\u001b[0m, in \u001b[0;36m_dynamize_fields\u001b[0;34m(cls, indicator_prefix, inheritence_strict)\u001b[0m\n\u001b[1;32m    311\u001b[0m all_parameters \u001b[39m=\u001b[39m [\n\u001b[1;32m    312\u001b[0m     p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m all_parameters \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m inspect\u001b[39m.\u001b[39mParameter\u001b[39m.\u001b[39mVAR_POSITIONAL\n\u001b[1;32m    313\u001b[0m ]\n\u001b[1;32m    315\u001b[0m all_parameters \u001b[39m=\u001b[39m [\n\u001b[1;32m    316\u001b[0m     p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m all_parameters \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m inspect\u001b[39m.\u001b[39mParameter\u001b[39m.\u001b[39mVAR_KEYWORD\n\u001b[1;32m    317\u001b[0m ]\n\u001b[0;32m--> 319\u001b[0m new_init\u001b[39m.\u001b[39m__signature__ \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mSignature(\n\u001b[1;32m    320\u001b[0m     all_parameters, return_annotation\u001b[39m=\u001b[39;49msig\u001b[39m.\u001b[39;49mreturn_annotation\n\u001b[1;32m    321\u001b[0m )\n\u001b[1;32m    323\u001b[0m \u001b[39m# 3. set the new init function\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m \u001b[39m=\u001b[39m new_init\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2964\u001b[0m, in \u001b[0;36mSignature.__init__\u001b[0;34m(self, parameters, return_annotation, __validate_parameters__)\u001b[0m\n\u001b[1;32m   2962\u001b[0m         \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   2963\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mduplicate parameter name: \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name)\n\u001b[0;32m-> 2964\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2966\u001b[0m         params[name] \u001b[39m=\u001b[39m param\n\u001b[1;32m   2967\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: duplicate parameter name: 'sample_a'"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import dycode.wrappers as dyw\n",
    "\n",
    "class A:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "@dyw.dynamize\n",
    "class B_P:\n",
    "    sample_a = dyw.field('A', is_class=True, constructor_arguments={'a': -1, 'b': -1})\n",
    "    def __init__(self, xp):\n",
    "        self.xp = xp\n",
    "\n",
    "@dyw.dynamize\n",
    "class B:\n",
    "    sample_bp = dyw.field('B_P', is_class=True, constructor_arguments={'xp': -1})\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "@dyw.dynamize\n",
    "class C:\n",
    "    sample_a = dyw.field('A', is_class=True, constructor_arguments={'a': 2, 'b': 5})\n",
    "    def __init__(self, a):\n",
    "        self.a = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "--------------------\n",
      "2\n",
      "101\n",
      "--------------------\n",
      "20\n",
      "--------------------\n",
      "11\n",
      "22\n",
      "33\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# a simple constructor\n",
    "sample1 = C(10)\n",
    "print(sample1.sample_a.a)\n",
    "print(\"--------------------\")\n",
    "# A little configuration\n",
    "sample1_5 = C(10, sample_a_dy_type='A', sample_a_dy_args={'b': 101})\n",
    "print(sample1_5.sample_a.a)\n",
    "print(sample1_5.sample_a.b)\n",
    "print(\"--------------------\")\n",
    "# changing a deeper field in the constructor\n",
    "sample2 = C(10, sample_a_dy_type='B', sample_a_dy_args={'x': 20})\n",
    "print(sample2.sample_a.x)\n",
    "print(\"--------------------\")\n",
    "# Changing even deeper field in the constructor\n",
    "sample3 = C(11, \n",
    "    sample_a_dy_type='B',\n",
    "    sample_a_dy_args={\n",
    "        'x': 22, \n",
    "        'sample_bp_dy_type': 'B_P', \n",
    "        'sample_bp_dy_args': {\n",
    "            'xp': 33\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(sample3.a)\n",
    "print(sample3.sample_a.x)\n",
    "print(sample3.sample_a.sample_bp.xp)\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_imaging' from 'PIL' (/usr/lib/python3/dist-packages/PIL/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mth\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightning\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m freeze_params, unfreeze_params, list_args\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdycode\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdy\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/__init__.py:32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__about__\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: E402, F401, F403\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__version__\u001b[39;00m \u001b[39mimport\u001b[39;00m version \u001b[39mas\u001b[39;00m __version__  \u001b[39m# noqa: E402, F401\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m \u001b[39mimport\u001b[39;00m storage  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningApp  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflow\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningFlow  \u001b[39m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/app/__init__.py:25\u001b[0m\n\u001b[1;32m     21\u001b[0m     _logger\u001b[39m.\u001b[39mpropagate \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m \u001b[39mimport\u001b[39;00m __about__  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m \u001b[39mimport\u001b[39;00m components  \u001b[39m# noqa: E402, F401\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__about__\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: E402, F401, F403\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(__about__, \u001b[39m\"\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/app/components/__init__.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserve\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgradio\u001b[39;00m \u001b[39mimport\u001b[39;00m ServeGradio\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserve\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython_server\u001b[39;00m \u001b[39mimport\u001b[39;00m Category, Image, Number, PythonServer, Text\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserve\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserve\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelInferenceAPI\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserve\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstreamlit\u001b[39;00m \u001b[39mimport\u001b[39;00m ServeStreamlit\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningTrainerScript, PyTorchLightningScriptRunner\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/app/components/serve/serve.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfastapi\u001b[39;00m \u001b[39mimport\u001b[39;00m FastAPI\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfastapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresponses\u001b[39;00m \u001b[39mimport\u001b[39;00m JSONResponse\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserve\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m _DESERIALIZER, _SERIALIZER\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwork\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningWork\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/app/components/serve/types/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserve\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m _SERIALIZER \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m: Image\u001b[39m.\u001b[39mserialize}\n\u001b[1;32m      4\u001b[0m _DESERIALIZER \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m: Image\u001b[39m.\u001b[39mdeserialize}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/app/components/serve/types/image.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m _is_pil_available():\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image \u001b[39mas\u001b[39;00m PILImage\n\u001b[1;32m     14\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mImage\u001b[39;00m(BaseType):\n\u001b[1;32m     15\u001b[0m     \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     16\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mdeserialize\u001b[39m(data: \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:69\u001b[0m\n\u001b[1;32m     60\u001b[0m MAX_IMAGE_PIXELS \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[39m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[39m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[39m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _imaging \u001b[39mas\u001b[39;00m core\n\u001b[1;32m     71\u001b[0m     \u001b[39mif\u001b[39;00m __version__ \u001b[39m!=\u001b[39m \u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     72\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCore version: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPillow version: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), __version__)\n\u001b[1;32m     76\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_imaging' from 'PIL' (/usr/lib/python3/dist-packages/PIL/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import typing as th\n",
    "import functools\n",
    "import lightning\n",
    "from .utils import freeze_params, unfreeze_params, list_args\n",
    "import dycode as dy\n",
    "from .criterion import Criterion\n",
    "import torch\n",
    "import types\n",
    "import dycode.wrappers as dyw\n",
    "\n",
    "\n",
    "@dyw.dynamize\n",
    "class TrainingModule(lightning.LightningModule):\n",
    "    \"\"\"\n",
    "    Generic Lightning Module for training MADE models.\n",
    "\n",
    "    Attributes:\n",
    "        model: the model to train\n",
    "        criterion: the criterion to use for training\n",
    "        batch_transform: the transform to apply to the inputs before forward pass\n",
    "    \"\"\"\n",
    "    criterion = dyw.field('lightning_toolbox.Criterion', is_class=True)\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        # model\n",
    "        model: th.Optional[torch.nn.Module] = None,\n",
    "        model_cls: th.Optional[str] = None,\n",
    "        model_args: th.Optional[dict] = None,\n",
    "        # input transforms [transform(inputs) -> torch.Tensor]\n",
    "        batch_transform: th.Optional[dy.FunctionDescriptor] = None,\n",
    "        # optimization configs [is_active(training_module, optimizer_idx) -> bool]\n",
    "        optimizer: th.Union[str, th.List[str]] = \"torch.optim.Adam\",\n",
    "        optimizer_is_active: th.Optional[th.Union[dy.FunctionDescriptor, th.List[dy.FunctionDescriptor]]] = None,\n",
    "        optimizer_parameters: th.Optional[th.Union[th.List[str], str]] = None,\n",
    "        optimizer_args: th.Optional[dict] = None,\n",
    "        # learning rate\n",
    "        lr: th.Union[th.List[float], float] = 1e-4,\n",
    "        # schedulers\n",
    "        scheduler: th.Optional[th.Union[str, th.List[str]]] = None,\n",
    "        scheduler_name: th.Optional[th.Union[str, th.List[str]]] = None,\n",
    "        scheduler_optimizer: th.Optional[th.Union[int, th.List[int]]] = None,\n",
    "        scheduler_args: th.Optional[th.Union[dict, th.List[dict]]] = None,\n",
    "        scheduler_interval: th.Union[str, th.List[str]] = \"epoch\",\n",
    "        scheduler_frequency: th.Union[int, th.List[int]] = 1,\n",
    "        scheduler_monitor: th.Optional[th.Union[str, th.List[str]]] = None,\n",
    "        # initialization settings\n",
    "        save_hparams: bool = True,\n",
    "        initialize_superclass: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the trainer.\n",
    "\n",
    "        Args:\n",
    "            model_cls: the class of the model to use (import path)\n",
    "            model_args: the arguments to pass to the model constructor\n",
    "            criterion_args: the arguments to pass to the criterion constructor\n",
    "            attack_args: the arguments to pass to the attacker constructor (PGDAttacker)\n",
    "            inputs_transform:\n",
    "                the transform function to apply to the inputs before forward pass, can be used for\n",
    "                applying dequantizations.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if initialize_superclass:\n",
    "            super().__init__()\n",
    "        if save_hparams:\n",
    "            self.save_hyperparameters(ignore=[\"model\"])\n",
    "        \n",
    "        self.batch_transform = batch_transform if batch_transform is not None else self.hparams.batch_transform\n",
    "\n",
    "        # optimizers and schedulers\n",
    "        if (optimizer if optimizer is not None else self.hparams.optimizer) is not None:\n",
    "            # optimizers\n",
    "            (\n",
    "                self.optimizer,\n",
    "                self.optimizer_is_active_descriptor,\n",
    "                self.optimizer_parameters,\n",
    "                self.optimizer_args,\n",
    "            ), optimizers_count = list_args(\n",
    "                optimizer if optimizer is not None else self.hparams.optimizer,\n",
    "                optimizer_is_active if optimizer_is_active is not None else self.hparams.optimizer_is_active,\n",
    "                optimizer_parameters if optimizer_parameters is not None else self.hparams.optimizer_parameters,\n",
    "                optimizer_args if optimizer_args is not None else self.hparams.optimizer_args,\n",
    "                return_length=True,\n",
    "            )\n",
    "            self.optimizer_is_active_descriptor = (\n",
    "                None\n",
    "                if all(i is None for i in self.optimizer_is_active_descriptor)\n",
    "                else self.optimizer_is_active_descriptor\n",
    "            )\n",
    "\n",
    "            # learning rates\n",
    "            self.lr = list_args(lr if lr is not None else self.hparams.lr, length=optimizers_count)\n",
    "\n",
    "            (\n",
    "                (\n",
    "                    self.scheduler,\n",
    "                    self.scheduler_name,\n",
    "                    self.scheduler_optimizer,\n",
    "                    self.scheduler_args,\n",
    "                    self.scheduler_interval,\n",
    "                    self.scheduler_frequency,\n",
    "                    self.scheduler_monitor,\n",
    "                ),\n",
    "                schedulers_count,\n",
    "            ) = list_args(\n",
    "                scheduler if scheduler is not None else self.hparams.scheduler,\n",
    "                scheduler_name if scheduler_name is not None else self.hparams.scheduler_name,\n",
    "                scheduler_optimizer if scheduler_optimizer is not None else self.hparams.scheduler_optimizer,\n",
    "                scheduler_args if scheduler_args is not None else self.hparams.scheduler_args,\n",
    "                scheduler_interval if scheduler_interval is not None else self.hparams.scheduler_interval,\n",
    "                scheduler_frequency if scheduler_frequency is not None else self.hparams.scheduler_frequency,\n",
    "                scheduler_monitor if scheduler_monitor is not None else self.hparams.scheduler_monitor,\n",
    "                return_length=True,\n",
    "            )\n",
    "            schedulers_count = schedulers_count if schedulers_count and self.scheduler[0] is not None else 0\n",
    "            if not schedulers_count:\n",
    "                (\n",
    "                    self.scheduler,\n",
    "                    self.scheduler_name,\n",
    "                    self.scheduler_optimizer,\n",
    "                    self.scheduler_args,\n",
    "                    self.scheduler_frequency,\n",
    "                    self.scheduler_interval,\n",
    "                    self.scheduler_monitor,\n",
    "                ) = (None, None, None, None, None, None, None)\n",
    "            if (schedulers_count == 1 and optimizers_count > 1) or (\n",
    "                schedulers_count > 0 and all(self.scheduler_optimizer[i] is None for i in range(schedulers_count))\n",
    "            ):\n",
    "                self.scheduler_optimizer = [i for j in range(schedulers_count) for i in range(optimizers_count)]\n",
    "                self.scheduler = [j for j in self.scheduler for i in range(optimizers_count)]\n",
    "                self.scheduler_name = [j for j in self.scheduler_name for i in range(optimizers_count)]\n",
    "                self.scheduler_args = [j for j in self.scheduler_args for i in range(optimizers_count)]\n",
    "                self.scheduler_interval = [j for j in self.scheduler_interval for i in range(optimizers_count)]\n",
    "                self.scheduler_frequency = [j for j in self.scheduler_frequency for i in range(optimizers_count)]\n",
    "                self.scheduler_monitor = [j for j in self.scheduler_monitor for i in range(optimizers_count)]\n",
    "            if schedulers_count:\n",
    "                for idx, name in enumerate(self.scheduler_name):\n",
    "                    param_name = self.optimizer_parameters[self.scheduler_optimizer[idx]]\n",
    "                    param_name = (\n",
    "                        f\"/{param_name}\"\n",
    "                        if param_name and isinstance(param_name, str)\n",
    "                        else f\"/{self.scheduler_optimizer[idx]}\"\n",
    "                    )\n",
    "                    self.scheduler_name[idx] = f\"lr_scheduler{param_name}/{name if name is not None else idx}\"\n",
    "                self.__scheduler_step_count = [0 for i in range(len(self.scheduler))]\n",
    "\n",
    "        # switching between manual and automatic optimization\n",
    "        if hasattr(self, \"optimizer_is_active_descriptor\") and self.optimizer_is_active_descriptor is not None:\n",
    "            self.automatic_optimization = False\n",
    "            self.training_step = types.MethodType(TrainingModule.training_step_manual, self)\n",
    "            self.__params_frozen = [False for i in range(optimizers_count)]\n",
    "            self.__params_state = [None for i in range(optimizers_count)]\n",
    "        else:\n",
    "            self.training_step = types.MethodType(TrainingModule.training_step_automatic, self)\n",
    "\n",
    "        # initialize the model\n",
    "        if (\n",
    "            model is not None\n",
    "            or model_args is not None\n",
    "            or model_cls is not None\n",
    "            or (hasattr(self.hparams, \"model_cls\") and self.hparams.model_cls is not None)\n",
    "            or (hasattr(self.hparams, \"model_args\") and self.hparams.model_args is not None)\n",
    "        ):\n",
    "            self.model = (\n",
    "                model if model is not None else dy.eval(self.hparams.model_cls)(**(self.hparams.model_args or dict()))\n",
    "            )\n",
    "\n",
    "    @functools.cached_property\n",
    "    def optimizer_is_active(self):\n",
    "        if self.optimizer_is_active_descriptor is None:\n",
    "            return None\n",
    "        return [\n",
    "            dy.eval(i, function_of_interest=\"is_active\", dynamic_args=True)\n",
    "            for i in self.optimizer_is_active_descriptor\n",
    "        ]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"Placeholder forward pass for the model\"\n",
    "        if hasattr(self, \"model\") and self.model is not None:\n",
    "            return self.model(inputs)\n",
    "        raise NotImplementedError(\"No model defined\")\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        batch: th.Optional[th.Any] = None,\n",
    "        batch_idx: th.Optional[int] = None,\n",
    "        optimizer_idx: th.Optional[int] = None,\n",
    "        name: str = \"train\",\n",
    "        transformed_batch: th.Optional[th.Any] = None,\n",
    "        transform_batch: bool = True,\n",
    "        return_results: bool = False,\n",
    "        return_factors: bool = False,\n",
    "        log_results: bool = True,\n",
    "        **kwargs,  # additional arguments to pass to the criterion and attacker\n",
    "    ):\n",
    "        \"\"\"Train or evaluate the model with the given batch.\n",
    "\n",
    "        Args:\n",
    "            batch: batch of data to train or evaluate with\n",
    "            batch_idx: index of the batch\n",
    "            optimizer_idx: index of the optimizer\n",
    "            name: name of the step (\"train\" or \"val\")\n",
    "\n",
    "        Returns:\n",
    "            None if the model is in evaluation mode, else a tensor with the training objective\n",
    "        \"\"\"\n",
    "        is_val = name == \"val\"\n",
    "        transformed_batch = self.process_batch(\n",
    "            batch, transformed_batch=transformed_batch, transform_batch=transform_batch\n",
    "        )\n",
    "        results, factors = self.criterion(batch=transformed_batch, training_module=self, return_factors=True, **kwargs)\n",
    "        if log_results:\n",
    "            self.log_step_results(results, factors, name)\n",
    "        if return_results:\n",
    "            return (results, factors) if return_factors else results\n",
    "        return results[\"loss\"] if not is_val else None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizers = [\n",
    "            self.__configure_optimizer(\n",
    "                opt_class=opt_cls, opt_args=opt_args, opt_base_lr=opt_base_lr, opt_parameters=opt_parameters\n",
    "            )\n",
    "            for opt_cls, opt_base_lr, opt_parameters, opt_args in zip(\n",
    "                self.optimizer,\n",
    "                self.lr,\n",
    "                self.optimizer_parameters,\n",
    "                self.optimizer_args,\n",
    "            )\n",
    "        ]\n",
    "        schedulers = (\n",
    "            [\n",
    "                self.__configure_scheduler(\n",
    "                    sched_class=sched_cls,\n",
    "                    sched_optimizer=optimizers[sched_optimizer if sched_optimizer is not None else 0],\n",
    "                    sched_args=sched_args,\n",
    "                    sched_interval=sched_interval,\n",
    "                    sched_frequency=sched_frequency,\n",
    "                    sched_monitor=sched_monitor,\n",
    "                    sched_name=sched_name,\n",
    "                )\n",
    "                for sched_cls, sched_optimizer, sched_args, sched_interval, sched_frequency, sched_monitor, sched_name in zip(\n",
    "                    self.scheduler,\n",
    "                    self.scheduler_optimizer,\n",
    "                    self.scheduler_args,\n",
    "                    self.scheduler_interval,\n",
    "                    self.scheduler_frequency,\n",
    "                    self.scheduler_monitor,\n",
    "                    self.scheduler_name,\n",
    "                )\n",
    "            ]\n",
    "            if self.scheduler\n",
    "            else None\n",
    "        )\n",
    "        if schedulers:\n",
    "            return (\n",
    "                dict(optimizer=optimizers[0], scheduler=schedulers[0])\n",
    "                if len(schedulers) == 1 and len(optimizers) == 1\n",
    "                else (\n",
    "                    optimizers,\n",
    "                    schedulers,\n",
    "                )\n",
    "            )\n",
    "        return optimizers\n",
    "\n",
    "    def __configure_optimizer(self, opt_class, opt_base_lr, opt_args, opt_parameters):\n",
    "        opt_class = dy.eval(opt_class)\n",
    "        opt_args = {\"lr\": opt_base_lr, **(opt_args if opt_args is not None else {})}\n",
    "        params = dy.eval(opt_parameters, context=self) if opt_parameters else self\n",
    "        if isinstance(params, torch.Tensor):\n",
    "            params = [params]\n",
    "        else:\n",
    "            params = params.parameters() if hasattr(params, \"parameters\") else params\n",
    "\n",
    "        opt = opt_class(\n",
    "            params,\n",
    "            **opt_args,\n",
    "        )\n",
    "        return opt\n",
    "\n",
    "    def __configure_scheduler(\n",
    "        self, sched_class, sched_optimizer, sched_args, sched_interval, sched_frequency, sched_monitor, sched_name\n",
    "    ):\n",
    "        sched_class = dy.eval(sched_class)\n",
    "        sched_args = sched_args or dict()\n",
    "        sched = sched_class(sched_optimizer, **sched_args)\n",
    "        sched_instance_dict = dict(\n",
    "            scheduler=sched,\n",
    "            interval=sched_interval,\n",
    "            frequency=sched_frequency,\n",
    "            name=sched_name,\n",
    "            reduce_on_plateau=isinstance(sched, torch.optim.lr_scheduler.ReduceLROnPlateau),\n",
    "        )\n",
    "        if sched_monitor is not None:\n",
    "            sched_instance_dict[\"monitor\"] = sched_monitor\n",
    "        return sched_instance_dict\n",
    "\n",
    "    @functools.cached_property\n",
    "    def batch_transform_function(self):\n",
    "        \"\"\"The transform function (callable) to apply to the inputs before forward pass.\n",
    "\n",
    "        Returns:\n",
    "            The compiled callable transform function if `self.inputs_transform` is provided else None\n",
    "        \"\"\"\n",
    "        return dy.eval(self.batch_transform, function_of_interest=\"transform\", dynamic_args=True, strict=False)\n",
    "\n",
    "    def process_batch(\n",
    "        self,\n",
    "        batch,\n",
    "        transformed_batch: th.Optional[th.Any] = None,\n",
    "        transform_batch: bool = True,\n",
    "    ):\n",
    "        \"Process the batch before forward pass\"\n",
    "        if transform_batch:\n",
    "            if self.batch_transform_function is not None:\n",
    "                transformed_batch = self.batch_transform_function(batch)\n",
    "            else:\n",
    "                transformed_batch = batch\n",
    "        else:\n",
    "            transformed_batch = batch\n",
    "        return transformed_batch\n",
    "\n",
    "    def log_step_results(self, results, factors, name: str = \"train\"):\n",
    "        \"Log the results of the step\"\n",
    "        is_val = name == \"val\"\n",
    "        # logging results\n",
    "        for item, value in results.items():\n",
    "            self.log(\n",
    "                f\"{item}/{name}\",\n",
    "                value.mean() if isinstance(value, torch.Tensor) else value,\n",
    "                on_step=not is_val,\n",
    "                on_epoch=is_val,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "                prog_bar=is_val and name == \"loss\",\n",
    "            )\n",
    "        # validation step only logs\n",
    "        if not is_val:\n",
    "            return\n",
    "\n",
    "        # logging factors\n",
    "        for item, value in factors.items():\n",
    "            self.log(\n",
    "                f\"factors/{item}/{name}\",\n",
    "                value.mean() if isinstance(value, torch.Tensor) else value,\n",
    "                on_step=not is_val,\n",
    "                on_epoch=is_val,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def training_step_automatic(self, batch, batch_idx, optimizer_idx=None, **kwargs):\n",
    "        \"Implementation for automatic Pytorch Lightning's training_step function\"\n",
    "        return self.step(batch, batch_idx, optimizer_idx, name=\"train\", **kwargs)\n",
    "\n",
    "    def manual_lr_schedulers_step(self, scheduler, scheduler_idx, **kwargs):\n",
    "        \"Implementation for manual Pytorch Lightning's lr_step function\"\n",
    "        frequency = self.scheduler_frequency[scheduler_idx]\n",
    "        if not frequency:\n",
    "            return\n",
    "        interval = self.scheduler_interval[scheduler_idx]\n",
    "        monitor = self.scheduler_monitor[scheduler_idx]\n",
    "\n",
    "        step = False\n",
    "        if interval == \"batch\":\n",
    "            self.__scheduler_step_count[scheduler_idx] = (1 + self.__scheduler_step_count[scheduler_idx]) % frequency\n",
    "            step = not self.__scheduler_step_count[scheduler_idx]\n",
    "        elif interval == \"epoch\":\n",
    "            step = self.trainer.is_last_batch and not (self.trainer.current_epoch % frequency)\n",
    "        if not step:\n",
    "            return\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            if monitor not in self.trainer.callback_metrics:\n",
    "                return  # no metric to monitor, skip scheduler step until metric is available in next loops\n",
    "            scheduler.step(self.trainer.callback_metrics[monitor])\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "    def training_step_manual(self, batch, batch_idx, **kwargs):\n",
    "        \"Implementation for manual training and optimization\"\n",
    "        optimizers = self.optimizers()\n",
    "        optimizers = optimizers if isinstance(optimizers, (list, tuple)) else [optimizers]\n",
    "        schedulers = self.lr_schedulers()\n",
    "        schedulers = schedulers if isinstance(schedulers, (list, tuple)) else ([schedulers] if schedulers else [])\n",
    "        optimizer_is_active = [\n",
    "            self.optimizer_is_active[i](training_module=self, optimizer_idx=i) for i in range(len(optimizers))\n",
    "        ]\n",
    "        # freezing/unfreezing the optimizer parameters\n",
    "        for optimizer_idx, optimizer in enumerate(optimizers):\n",
    "            if optimizer_is_active[optimizer_idx] and self.__params_frozen[optimizer_idx]:\n",
    "                unfreeze_params(optimizer=optimizer, old_states=self.__params_state[optimizer_idx])\n",
    "                self.__params_frozen[optimizer_idx] = False\n",
    "            elif not optimizer_is_active[optimizer_idx] and not self.__params_frozen[optimizer_idx]:\n",
    "                self.__params_state[optimizer_idx] = freeze_params(optimizer=optimizer)\n",
    "                self.__params_frozen[optimizer_idx] = True\n",
    "        loss = self.step(batch, batch_idx, None, name=\"train\")\n",
    "        if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "            self.manual_backward(loss)\n",
    "        else:\n",
    "            return loss\n",
    "        for optimizer_idx, optimizer in enumerate(optimizers):\n",
    "            if optimizer_is_active[optimizer_idx]:\n",
    "                optimizer.step()  # todo: add support for LBFGS optimizers via closures\n",
    "                optimizer.zero_grad()  # todo: move to before the backward call and add support for gradient accumulation\n",
    "        # following pytorch>=1.1.0 conventions, calling scheduler.step after optimizer.step\n",
    "        # visit the docs for more details https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "        for idx, scheduler in enumerate(schedulers):\n",
    "            if optimizer_is_active[self.scheduler_optimizer[idx]]:\n",
    "                self.manual_lr_schedulers_step(scheduler=scheduler, scheduler_idx=idx)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"Pytorch Lightning's validation_step function\"\n",
    "        return self.step(batch, batch_idx, name=\"val\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
